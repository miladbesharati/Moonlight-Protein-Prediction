{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4342cb-d9b7-4cc8-8648-7f39cb36c8d8",
   "metadata": {},
   "source": [
    "# TF‚ÄìIDF Feature Extraction for Protein Sequences\n",
    "## This script reads MP and Non-MP sequences from CSV files, computes k-mer-based TF‚ÄìIDF features (using amino acid tokens), and outputs a normalized TF‚ÄìIDF feature matrix with protein labels for downstream machine learning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadfa56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Superzchen/iFeature.git\n",
    "\n",
    "%cd /content/iFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ea16d1-c94d-4643-ae27-868c172f30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "INPUT_CSV_FILE = '/content/drive/MyDrive/MP_Prediction_MB/Shirafkan/Non_MP_final_clean.csv'\n",
    "TEMP_FASTA_FILE = '/content/temp_sequences.fasta'\n",
    "BASE_OUTPUT_DIR = '/content/features'\n",
    "COMBINED_OUTPUT_FILE = '/content/drive/MyDrive/MP_Prediction_MB/Shirafkan/BF/BF_Non_MP1.csv'\n",
    "IFEAURE_SCRIPT_PATH = '/content/iFeature/iFeature.py'\n",
    "\n",
    "os.makedirs(BASE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def convert_csv_to_fasta(csv_path, fasta_path):\n",
    "    \"\"\"\n",
    "    Reads a CSV with 'protein_name' and 'sequence' columns and writes a FASTA file.\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Converting {os.path.basename(csv_path)} to FASTA format...\")\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "\n",
    "        if 'protein_name' not in df.columns or 'sequence' not in df.columns:\n",
    "            raise ValueError(\"CSV file must contain 'protein_name' and 'sequence' columns.\")\n",
    "\n",
    "        with open(fasta_path, 'w') as f_out:\n",
    "            for index, row in df.iterrows():\n",
    "                protein_name = row['protein_name']\n",
    "                sequence = row['sequence']\n",
    "                f_out.write(f\">{protein_name}\\n\")\n",
    "                f_out.write(f\"{sequence}\\n\")\n",
    "        print(f\" Conversion successful. FASTA file created at: {fasta_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\" Error during CSV to FASTA conversion: {e}\")\n",
    "        return False\n",
    "\n",
    "def run_ifeature(feature_type, fasta_file, output_dir, timeout=900):\n",
    "    \"\"\"Run iFeature extraction for a single feature type.\"\"\"\n",
    "    output_file = os.path.join(output_dir, f'{feature_type}_features.tsv')\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['python', IFEAURE_SCRIPT_PATH, '--file', fasta_file, '--type', feature_type, '--out', output_file],\n",
    "            capture_output=True, text=True, check=True, timeout=timeout\n",
    "        )\n",
    "        return feature_type, True, \"Success\"\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        return feature_type, False, f\"Error: {e.stderr.strip()}\"\n",
    "    except subprocess.TimeoutExpired:\n",
    "        return feature_type, False, f\"Timeout after {timeout} seconds\"\n",
    "    except Exception as e:\n",
    "        return feature_type, False, f\"An unexpected exception occurred: {str(e)}\"\n",
    "\n",
    "def extract_features_parallel(feature_types, fasta_file, output_dir, max_workers=12):\n",
    "    \"\"\"Extract features in parallel using a thread pool.\"\"\"\n",
    "    print(f\"üöÄ Starting feature extraction for {len(feature_types)} types using up to {max_workers} workers...\")\n",
    "    results = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_feature = {\n",
    "            executor.submit(run_ifeature, ft, fasta_file, output_dir): ft\n",
    "            for ft in feature_types\n",
    "        }\n",
    "        for future in as_completed(future_to_feature):\n",
    "            feature_type = future_to_feature[future]\n",
    "            try:\n",
    "                feature, success, message = future.result()\n",
    "                results[feature] = (success, message)\n",
    "                status = \"‚úÖ\" if success else \"‚ùå\"\n",
    "                print(f\"  {status} {feature:<15} | {message}\")\n",
    "            except Exception as e:\n",
    "                results[feature_type] = (False, f\"Future submission failed: {str(e)}\")\n",
    "                print(f\"  ‚ùå {feature_type:<15} | Future submission failed\")\n",
    "    return results\n",
    "\n",
    "def read_feature_file(file_path, feature_type):\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t')\n",
    "        if df.empty:\n",
    "            return None\n",
    "        id_col = df.columns[0]\n",
    "        df = df.rename(columns={id_col: 'base_id'})\n",
    "        df['base_id'] = df['base_id'].astype(str).apply(\n",
    "            lambda x: x.split('|')[1] if '|' in x and len(x.split('|')) > 1 else x\n",
    "        )\n",
    "        feature_cols = [col for col in df.columns if col != 'base_id']\n",
    "        new_cols = {col: f\"{feature_type}_{col}\" for col in feature_cols}\n",
    "        df = df.rename(columns=new_cols)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {os.path.basename(file_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "def combine_features_parallel(output_dir, success_results):\n",
    "    \"\"\"Combine successfully generated feature files.\"\"\"\n",
    "    files_to_process = [f\"{ft}_features.tsv\" for ft, (success, msg) in success_results.items() if success]\n",
    "    if not files_to_process:\n",
    "        print(\"No feature files were successfully generated to combine.\")\n",
    "        return []\n",
    "    print(f\"\\nüìö Reading and processing {len(files_to_process)} feature files...\")\n",
    "    feature_dfs = []\n",
    "    with ThreadPoolExecutor(max_workers=min(8, len(files_to_process))) as executor:\n",
    "        future_to_file = {}\n",
    "        for filename in files_to_process:\n",
    "            file_path = os.path.join(output_dir, filename)\n",
    "            feature_type = filename.replace('_features.tsv', '')\n",
    "            future = executor.submit(read_feature_file, file_path, feature_type)\n",
    "            future_to_file[future] = filename\n",
    "        for future in as_completed(future_to_file):\n",
    "            filename = future_to_file[future]\n",
    "            try:\n",
    "                df = future.result()\n",
    "                if df is not None and not df.empty:\n",
    "                    feature_dfs.append(df)\n",
    "                    print(f\"  ‚úÖ Processed {filename}\")\n",
    "                elif df is None:\n",
    "                    pass\n",
    "                else:\n",
    "                    print(f\"  ‚ö†Ô∏è Skipped {filename}: No valid data found.\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Error processing future for {filename}: {e}\")\n",
    "    return feature_dfs\n",
    "\n",
    "def merge_dataframes(dataframes):\n",
    "    \"\"\"Merge a list of dataframes on the 'base_id' column.\"\"\"\n",
    "    if not dataframes:\n",
    "        return pd.DataFrame()\n",
    "    merged_df = dataframes[0]\n",
    "    for df in dataframes[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on='base_id', how='outer')\n",
    "    return merged_df\n",
    "\n",
    "def main():\n",
    "\n",
    "    try:\n",
    "        if not convert_csv_to_fasta(INPUT_CSV_FILE, TEMP_FASTA_FILE):\n",
    "            print(\"\\nExiting script due to conversion failure.\")\n",
    "            return\n",
    "\n",
    "        feature_types = [\n",
    "           'AAC', 'DPC', 'TPC', 'CTDC', 'CTDT', 'CTDD',\n",
    "           'CTriad', 'GAAC', 'GDPC', 'GTPC', 'PAAC', 'APAAC',\n",
    "           'SOCNumber', 'QSOrder', 'NMBroto', 'Moran', 'Geary'\n",
    "        ]\n",
    "        extraction_results = extract_features_parallel(feature_types, TEMP_FASTA_FILE, BASE_OUTPUT_DIR)\n",
    "\n",
    "        # Step 3: Combine and merge the results (no changes here)\n",
    "        list_of_dfs = combine_features_parallel(BASE_OUTPUT_DIR, extraction_results)\n",
    "        if not list_of_dfs:\n",
    "            print(\"\\nNo dataframes to merge. Exiting.\")\n",
    "            return\n",
    "\n",
    "        print(\"\\n Merging all features into a single dataframe...\")\n",
    "        combined_data = merge_dataframes(list_of_dfs)\n",
    "        if combined_data.empty:\n",
    "            print(\"Merging resulted in an empty dataframe. Check input files.\")\n",
    "            return\n",
    "\n",
    "        combined_data = combined_data.fillna(0)\n",
    "        combined_data.to_csv(COMBINED_OUTPUT_FILE, index=False)\n",
    "        print(\"\\n--- All tasks complete! ---\")\n",
    "        print(f\"Final combined dataset shape: {combined_data.shape[0]} sequences, {combined_data.shape[1]} columns (including base_id)\")\n",
    "        print(f\"Results saved to: {COMBINED_OUTPUT_FILE}\")\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(TEMP_FASTA_FILE):\n",
    "            os.remove(TEMP_FASTA_FILE)\n",
    "            print(f\"\\nüßπ Cleaned up temporary file: {TEMP_FASTA_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b4f9ee-5af0-4a2c-afe4-0b0979813d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make headers.txt based on chosen features\n",
    "\n",
    "with open('/content/headers.txt', 'w') as file:\n",
    "    for header in headers_list:\n",
    "        file.write(header + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b86dc4-300e-4432-8d6d-29794764f775",
   "metadata": {},
   "outputs": [],
   "source": [
    "socnumber_indices = list(range(305, 365))\n",
    "qsorder_indices = list(range(445, 545))\n",
    "moran_indices = list(range(1483, 1723))\n",
    "nmbroto_indices = list(range(1723, 1963))\n",
    "geary_indices = list(range(1963, 2203))\n",
    "\n",
    "selected_indices = socnumber_indices + qsorder_indices + moran_indices + nmbroto_indices + geary_indices\n",
    "\n",
    "selected_features = [feature_names['cleaned_headers'][i-2] for i in selected_indices]\n",
    "\n",
    "# adjusted_indices = [i - 2 for i in selected_indices]\n",
    "\n",
    "selected_df_names = df[selected_features]\n",
    "\n",
    "# names_from_indices = df.columns[[i - 2 for i in selected_indices]].tolist()\n",
    "\n",
    "# match = names_from_indices == selected_features\n",
    "# print(\"Do the indices correspond to the features? \", match)\n",
    "\n",
    "# if not match:\n",
    "#     mismatches = [(idx, name_idx, name_feat) for idx, name_idx, name_feat in zip(selected_indices, names_from_indices, selected_features) if name_idx != name_feat]\n",
    "#     print(\"Mismatches (1-based index, name from index, name from features):\", mismatches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282b1ac-29bb-4a6d-97fa-3caf05f207e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_features = [col for col in selected_features if col in df.columns]\n",
    "missing_features = [col for col in selected_features if col not in df.columns]\n",
    "\n",
    "print(f\"Found {len(existing_features)} existing features\")\n",
    "print(f\"Missing {len(missing_features)} features: {missing_features}\")\n",
    "\n",
    "# Get existing columns\n",
    "selected_df_names = df[existing_features].copy()\n",
    "\n",
    "# Add missing columns filled with zeros\n",
    "for feature in missing_features:\n",
    "    selected_df_names[feature] = 0\n",
    "\n",
    "# Reorder columns to match the original selected_features order\n",
    "selected_df_names = selected_df_names[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b14209-595d-4afb-8e14-64f4e62302b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df_names.to_csv(r'/content/drive/MyDrive/MP_Prediction_MB/MPFit/BF/BF_Non_MP_selected.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
