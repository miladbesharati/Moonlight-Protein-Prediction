{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f4342cb-d9b7-4cc8-8648-7f39cb36c8d8",
   "metadata": {},
   "source": [
    "# Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30df2096-7c14-4cd6-944f-e3b55c668f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Readfastafile(fasta_path):\n",
    "\n",
    "  seq_dict = {}\n",
    "  cnt=0\n",
    "# Read FASTA and store each sequence string as a key\n",
    "  for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "    sequence = str(record.seq)\n",
    "    seq_dict[sequence] = None  # or any value you want\n",
    "    cnt=cnt+1\n",
    "# Example: print number of sequences and first one\n",
    "  print(cnt)\n",
    "  return(seq_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f751309-ce3f-474f-8986-95aa651210a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DefineTest1(x0_test,x1_test,Indexsample,y_pred):\n",
    "  Pred=[]\n",
    "  Label=[]\n",
    "  for i in range(len(x0_test)):\n",
    "    cnt0Pred=0\n",
    "    cnt1Pred=0\n",
    "    cnt1=0\n",
    "    cnt0=0\n",
    "    for j in range(len(Indexsample)):\n",
    "      triplet=Indexsample[j].split(\",\")   #pair[0] is index from test, pair[1] identifies which test x0_test or s1_test, pair[2] identifies which x0\n",
    "      if int(triplet[0])==i: #intdex test\n",
    "        if int(triplet[1])==0: #type test\n",
    "          if int(triplet[2])==1: #type train\n",
    "              cnt1Pred=cnt1Pred+y_pred[j]\n",
    "              cnt1=cnt1+1\n",
    "          if int(triplet[2])==0: #type train\n",
    "              cnt1Pred=cnt1Pred+(1-y_pred[j])\n",
    "              cnt1=cnt1+1\n",
    "    pos=cnt1Pred/cnt1\n",
    "\n",
    "    Pred.append(pos)\n",
    "    Label.append(0)\n",
    "\n",
    "  for i in range(len(x1_test)):\n",
    "    cnt0Pred=0\n",
    "    cnt1Pred=0\n",
    "    cnt1=0\n",
    "    cnt0=0\n",
    "    for j in range(len(Indexsample)):\n",
    "      triplet=Indexsample[j].split(\",\")   #pair[0] is index from test, pair[1] identifies which test x0_test or s1_test, pair[2] identifies which x0\n",
    "      if int(triplet[0])==i: #intdex test\n",
    "        if int(triplet[1])==1: #type test\n",
    "          if int(triplet[2])==0: #type train\n",
    "              cnt1Pred=cnt1Pred+(1-y_pred[j])\n",
    "              cnt1=cnt1+1\n",
    "          if int(triplet[2])==1: #type train\n",
    "              cnt1Pred=cnt1Pred+y_pred[j]\n",
    "              cnt1=cnt1+1\n",
    "    pos=cnt1Pred/cnt1\n",
    "    Pred.append(pos)\n",
    "    Label.append(1)\n",
    "  return(Pred,Label)\n",
    "\n",
    "def DefineTest2(x0_test,x1_test,Indexsample,y_pred):\n",
    "  Pred=[]\n",
    "  Pred1=[]\n",
    "  Label=[]\n",
    "  for i in range(len(x0_test)):\n",
    "    cnt0Pred=0\n",
    "    cnt1Pred=0\n",
    "    cnt1=0\n",
    "    cnt0=0\n",
    "    for j in range(len(Indexsample)):\n",
    "      triplet=Indexsample[j].split(\",\")   #pair[0] is index from test, pair[1] identifies which test x0_test or s1_test, pair[2] identifies which x0\n",
    "      if int(triplet[0])==i: #intdex test\n",
    "        if int(triplet[1])==0: #type test\n",
    "          if int(triplet[2])==1: #type train\n",
    "              cnt1Pred=cnt1Pred+y_pred[j]\n",
    "              cnt1=cnt1+1\n",
    "          if int(triplet[2])==0: #type train\n",
    "              cnt0Pred=cnt0Pred+y_pred[j]\n",
    "              cnt0=cnt0+1\n",
    "    if cnt0Pred/cnt0>cnt1Pred/cnt1:\n",
    "      Pred.append(0)\n",
    "    else:\n",
    "      Pred.append(1)\n",
    "    pos=(1-(cnt0Pred/cnt0))+(cnt1Pred/cnt1)\n",
    "    Pred1.append(pos/2)\n",
    "    Label.append(0)\n",
    "\n",
    "  for i in range(len(x1_test)):\n",
    "    cnt0Pred=0\n",
    "    cnt1Pred=0\n",
    "    cnt1=0\n",
    "    cnt0=0\n",
    "    for j in range(len(Indexsample)):\n",
    "      triplet=Indexsample[j].split(\",\")   #pair[0] is index from test, pair[1] identifies which test x0_test or s1_test, pair[2] identifies which x0\n",
    "      if int(triplet[0])==i: #intdex test\n",
    "        if int(triplet[1])==1: #type test\n",
    "          if int(triplet[2])==0: #type train\n",
    "              cnt0Pred=cnt0Pred+y_pred[j]\n",
    "              cnt0=cnt0+1\n",
    "          if int(triplet[2])==1: #type train\n",
    "              cnt1Pred=cnt1Pred+y_pred[j]\n",
    "              cnt1=cnt1+1\n",
    "    if cnt0Pred/cnt0>cnt1Pred/cnt1:\n",
    "      Pred.append(0)\n",
    "    else:\n",
    "      Pred.append(1)\n",
    "    pos=(1-(cnt0Pred/cnt0))+(cnt1Pred/cnt1)\n",
    "    Pred1.append(pos/2)\n",
    "    Label.append(1)\n",
    "  return(Pred1,Pred,Label)\n",
    "\n",
    "def DefineTest3(x0_test,x1_test,Indexsample,y_pred):\n",
    "  Pred=[]\n",
    "  Pred1=[]\n",
    "  Label=[]\n",
    "  for i in range(len(x0_test)):\n",
    "    cnt0Pred=0\n",
    "    cnt1Pred=0\n",
    "    cnt1=0\n",
    "    cnt0=0\n",
    "    for j in range(len(Indexsample)):\n",
    "      triplet=Indexsample[j].split(\",\")   #pair[0] is index from test, pair[1] identifies which test x0_test or s1_test, pair[2] identifies which x0\n",
    "      if int(triplet[0])==i: #intdex test\n",
    "        if int(triplet[1])==0: #type test\n",
    "          if int(triplet[2])==1: #type train\n",
    "              cnt1Pred=cnt1Pred+y_pred[j]\n",
    "              cnt1=cnt1+1\n",
    "          if int(triplet[2])==0: #type train\n",
    "              cnt0Pred=cnt0Pred+y_pred[j]\n",
    "              cnt0=cnt0+1\n",
    "    if cnt0Pred/cnt0>cnt1Pred/cnt1:\n",
    "      Pred.append(0)\n",
    "    else:\n",
    "      Pred.append(1)\n",
    "    pos=(1-(cnt0Pred/cnt0))+(cnt1Pred/cnt1)\n",
    "    Pred1.append(pos/2)\n",
    "    Label.append(0)\n",
    "\n",
    "  for i in range(len(x1_test)):\n",
    "    cnt0Pred=0\n",
    "    cnt1Pred=0\n",
    "    cnt1=0\n",
    "    cnt0=0\n",
    "    for j in range(len(Indexsample)):\n",
    "      triplet=Indexsample[j].split(\",\")   #pair[0] is index from test, pair[1] identifies which test x0_test or s1_test, pair[2] identifies which x0\n",
    "      if int(triplet[0])==i: #intdex test\n",
    "        if int(triplet[1])==1: #type test\n",
    "          if int(triplet[2])==0: #type train\n",
    "              cnt0Pred=cnt0Pred+y_pred[j]\n",
    "              cnt0=cnt0+1\n",
    "          if int(triplet[2])==1: #type train\n",
    "              cnt1Pred=cnt1Pred+y_pred[j]\n",
    "              cnt1=cnt1+1\n",
    "    if cnt0Pred/cnt0>cnt1Pred/cnt1:\n",
    "      Pred.append(0)\n",
    "    else:\n",
    "      Pred.append(1)\n",
    "    pos=(1-(cnt0Pred/cnt0))+(cnt1Pred/cnt1)\n",
    "    Pred1.append(pos/2)\n",
    "    Label.append(1)\n",
    "  return(Pred1,Pred,Label) #AUC,ACC,true\n",
    "\n",
    "\n",
    "def Add_max(max,value,L):\n",
    "  if(len(max)<L):\n",
    "    max.append(value)\n",
    "  else:\n",
    "    min_index = max.index(min(max))\n",
    "    if max[min_index]<value:\n",
    "      max[min_index]=value\n",
    "  return(max)\n",
    "\n",
    "\n",
    "def DefineTest4(x0_test,x1_test,Indexsample,y_pred,L0,L1):\n",
    "  Pred=[]\n",
    "  Pred1=[]\n",
    "  Label=[]\n",
    "  for i in range(len(x0_test)):\n",
    "    cnt0Pred=0\n",
    "    cnt1Pred=0\n",
    "    cnt1=0\n",
    "    cnt0=0\n",
    "    max1=[]\n",
    "    max0=[]\n",
    "    for j in range(len(Indexsample)):\n",
    "      triplet=Indexsample[j].split(\",\")   #pair[0] is index from test, pair[1] identifies which test x0_test or s1_test, pair[2] identifies which x0\n",
    "      if int(triplet[0])==i: #intdex test\n",
    "        if int(triplet[1])==0: #type test\n",
    "          if int(triplet[2])==1: #type train\n",
    "              max1=Add_max(max1,y_pred[j],L1)\n",
    "          if int(triplet[2])==0: #type train\n",
    "              max0=Add_max(max0,y_pred[j],L0)\n",
    "    avg1 = np.mean(max1)\n",
    "    avg0 = np.mean(max0)\n",
    "    if avg0>avg1:\n",
    "      Pred.append(0)\n",
    "    else:\n",
    "      Pred.append(1)\n",
    "    Pred1.append((avg1+1-avg0)/2)\n",
    "    Label.append(0)\n",
    "\n",
    "  for i in range(len(x1_test)):\n",
    "    cnt0Pred=0\n",
    "    cnt1Pred=0\n",
    "    cnt1=0\n",
    "    cnt0=0\n",
    "    max0=[]\n",
    "    max1=[]\n",
    "    for j in range(len(Indexsample)):\n",
    "      triplet=Indexsample[j].split(\",\")   #pair[0] is index from test, pair[1] identifies which test x0_test or s1_test, pair[2] identifies which x0\n",
    "      if int(triplet[0])==i: #intdex test\n",
    "        if int(triplet[1])==1: #type test\n",
    "          if int(triplet[2])==1: #type train\n",
    "              max1=Add_max(max1,y_pred[j],L1)\n",
    "          if int(triplet[2])==0: #type train\n",
    "              max0=Add_max(max0,y_pred[j],L0)\n",
    "    avg1 = np.mean(max1)\n",
    "    avg0 = np.mean(max0)\n",
    "    if avg0>avg1:\n",
    "      Pred.append(0)\n",
    "    else:\n",
    "      Pred.append(1)\n",
    "    Pred1.append((avg1+1-avg0)/2)\n",
    "    Label.append(1)\n",
    "  return(Pred1,Pred,Label)\n",
    "\n",
    "\n",
    "def RealTest(X,x0_train,x1_train):\n",
    "  dataset_test=[] # concat train\n",
    "  Indexsample=[]\n",
    "  for i in range(len(X)):\n",
    "    for j in range(len(x0_train)):\n",
    "      dataset_test.append(np.concatenate((X[i],x0_train[j]), axis=None))\n",
    "      Indexsample.append(str(i)+\",0\")   #index in test,x0_test,x0_train\n",
    "  for i in range(len(X)):\n",
    "    for j in range(len(x1_train)):\n",
    "      dataset_test.append(np.concatenate((X[i],x1_train[j]), axis=None))\n",
    "      Indexsample.append(str(i)+\",1\")   #index in test,x0_test,x1_train\n",
    "  return(np.asarray(dataset_test),Indexsample)\n",
    "\n",
    "def RealPredict1(X,test_pred,Indexsample):\n",
    "  Pred=[]\n",
    "  for i in range(len(X)):\n",
    "    cnt0Pred=0\n",
    "    cnt1Pred=0\n",
    "\n",
    "    for j in range(len(Indexsample)):\n",
    "      triplet=Indexsample[j].split(\",\")   #pair[0] is index from test, pair[1] identifies which test x0_test or s1_test, pair[2] identifies which x0\n",
    "\n",
    "      if int(triplet[0])==i: #intdex test\n",
    "          if int(triplet[1])==1: #type train\n",
    "            if test_pred[j]>=0.5: #type prediction\n",
    "              cnt1Pred=cnt1Pred+1\n",
    "            else:\n",
    "              cnt0Pred=cnt0Pred+1\n",
    "          if int(triplet[1])==0: #type train\n",
    "            if test_pred[j]>=0.5: #type prediction\n",
    "              cnt0Pred=cnt0Pred+1\n",
    "            else:\n",
    "              cnt1Pred=cnt1Pred+1\n",
    "\n",
    "    if cnt0Pred>cnt1Pred:\n",
    "      Pred.append(cnt0Pred)\n",
    "    else:\n",
    "      Pred.append(cnt1Pred)\n",
    "  return(Pred)\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "\n",
    "#AUC,ACC,true\n",
    "def Accuracy1(label_test, pred,pred1):\n",
    "    acc = accuracy_score(label_test, pred)\n",
    "    precision = precision_score(label_test, pred)\n",
    "    sensitivity = recall_score(label_test, pred)  # same as recall\n",
    "    tn, fp, fn, tp = confusion_matrix(label_test, pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = f1_score(label_test, pred)\n",
    "    auc = roc_auc_score(label_test, pred1)  # use probabilities here\n",
    "    aupr = average_precision_score(label_test, pred1)  # use probabilities\n",
    "\n",
    "    print(\"ACC  pre  recall spe F1  Auc  Aupr TN  FP  FN  TP\")\n",
    "    print(f\"{acc:.4f}  {precision:.4f}  {sensitivity:.4f}  {specificity:.4f}  \"\n",
    "          f\"{f1:.4f}  {auc:.4f}  {aupr:.4f}  {tn}  {fp}  {fn}  {tp}\")\n",
    "\n",
    "def Accuracy(label_test, pred,T):\n",
    "    pred1=np.copy(pred)\n",
    "    pred1 = (pred1 > T).astype(int)\n",
    "\n",
    "    acc = accuracy_score(label_test, pred1)\n",
    "    precision = precision_score(label_test, pred1)\n",
    "    sensitivity = recall_score(label_test, pred1)  # same as recall\n",
    "    tn, fp, fn, tp = confusion_matrix(label_test, pred1).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = f1_score(label_test, pred1)\n",
    "    auc = roc_auc_score(label_test, pred)  # use probabilities here\n",
    "    aupr = average_precision_score(label_test, pred)  # use probabilities\n",
    "\n",
    "    print(\"ACC  pre  recall spe F1  Auc  Aupr TN  FP  FN  TP\")\n",
    "    print(f\"{acc:.4f}  {precision:.4f}  {sensitivity:.4f}  {specificity:.4f}  \"\n",
    "          f\"{f1:.4f}  {auc:.4f}  {aupr:.4f}  {tn}  {fp}  {fn}  {tp}\")\n",
    "\n",
    "\n",
    "def makedatasetPos(data):\n",
    "  dataset=[]\n",
    "  label=[]\n",
    "  for i in range(len(data)):\n",
    "    for j in range(i+1,len(data)):\n",
    "      dataset.append(np.concatenate((data[i],data[j]), axis=None))\n",
    "      label.append(1)\n",
    "      dataset.append(np.concatenate((data[j],data[i]), axis=None))\n",
    "      label.append(1)\n",
    "  return(dataset,label)\n",
    "\n",
    "def makedatasetNeg(data1,data2):\n",
    "  dataset=[]\n",
    "  label=[]\n",
    "  for i in range(len(data1)):\n",
    "    for j in range(len(data2)):\n",
    "      dataset.append(np.concatenate((data1[i],data2[j]), axis=None))\n",
    "      label.append(0)\n",
    "      dataset.append(np.concatenate((data2[j],data1[i]), axis=None))\n",
    "      label.append(0)\n",
    "  return(dataset,label)\n",
    "\n",
    "def Makedataset(X1,X0):\n",
    "  Y1 = [1 for i in range(len(X1))]\n",
    "  Y0 = [0 for i in range(len(X0))]\n",
    "  x0_train,x0_test,y0_train,y0_test=train_test_split(X0,Y0,test_size=0.1,shuffle=True)\n",
    "  x1_train,x1_test,y1_train,y1_test=train_test_split(X1,Y1,test_size=0.1,shuffle=True)\n",
    "  dataset1p,label1p=makedatasetPos(x1_train)\n",
    "  dataset0p,label0p=makedatasetPos(x0_train)\n",
    "  datasetn,labeln=makedatasetNeg(x0_train,x1_train)\n",
    "  main_train1p=np.asarray(dataset1p)\n",
    "  main_train0p=np.asarray(dataset0p)\n",
    "  main_trainn=np.asarray(datasetn)\n",
    "  main_labeln=np.asarray(labeln)\n",
    "  main_label1p=np.asarray(label1p)\n",
    "  main_label0p=np.asarray(label0p)\n",
    "  return( main_train1p,main_train0p,main_trainn,main_labeln, main_label1p, main_label0p,x0_test,x1_test,x0_train,x1_train)\n",
    "\n",
    "\n",
    "def Makedatasetfor5fold(x1_train,x0_train):\n",
    "  dataset1p,label1p=makedatasetPos(x1_train)\n",
    "  dataset0p,label0p=makedatasetPos(x0_train)\n",
    "  datasetn,labeln=makedatasetNeg(x0_train,x1_train)\n",
    "  main_train1p=np.asarray(dataset1p)\n",
    "  main_train0p=np.asarray(dataset0p)\n",
    "  main_trainn=np.asarray(datasetn)\n",
    "  main_labeln=np.asarray(labeln)\n",
    "  main_label1p=np.asarray(label1p)\n",
    "  main_label0p=np.asarray(label0p)\n",
    "  return( main_train1p,main_train0p,main_trainn,main_labeln, main_label1p, main_label0p)\n",
    "\n",
    "def MakedatasetTotal(x1_train,x0_train):\n",
    "  dataset1p,label1p=makedatasetPos(x1_train)\n",
    "  dataset0p,label0p=makedatasetPos(x0_train)\n",
    "  datasetn,labeln=makedatasetNeg(x0_train,x1_train)\n",
    "  main_train1p=np.asarray(dataset1p)\n",
    "  main_train0p=np.asarray(dataset0p)\n",
    "  main_trainn=np.asarray(datasetn)\n",
    "  main_labeln=np.asarray(labeln)\n",
    "  main_label1p=np.asarray(label1p)\n",
    "  main_label0p=np.asarray(label0p)\n",
    "  return( main_train1p,main_train0p,main_trainn,main_labeln, main_label1p, main_label0p)\n",
    "\n",
    "def MakeDatasetforTrain(P1,P0,N,LP1,LP0,LN):\n",
    "  TempP0=[]\n",
    "  TempL0=[]\n",
    "  i=0\n",
    "  while(i<len(P1)):\n",
    "    l=random.randint(0,len(P0)-1)\n",
    "    TempP0.append(P0[l])\n",
    "    TempL0.append(LP0[l])\n",
    "    i=i+1\n",
    "  print('P1',len(P1), 'P0', len(TempP0))\n",
    "  train=np.copy(np.append(P1,TempP0,axis=0))\n",
    "  label=np.copy(np.append(LP1,TempL0))\n",
    "\n",
    "  TempN=[]\n",
    "  TempLN=[]\n",
    "  while(len(TempN)<len(train)):\n",
    "    l=random.randint(0,len(N)-1)\n",
    "    TempN.append(N[l])\n",
    "    TempLN.append(LN[l])\n",
    "\n",
    "  train=np.copy(np.append(train,TempN,axis=0))\n",
    "  label=np.copy(np.append(label,TempLN))\n",
    "\n",
    "  return(train,label)\n",
    "\n",
    "def training(main_train1p,main_train0p,main_trainn,main_labeln, main_label1p, main_label0p,Epoch,siamese_net):\n",
    "\n",
    "  history=History()\n",
    "  train,label=MakeDatasetforTrain(main_train1p,main_train0p,main_trainn,main_label1p,main_label0p,main_labeln)\n",
    "  main_trainL1=np.array(train[:,0:int(len(train[0])/2)])\n",
    "  main_trainR1=np.array(train[:,int(len(train[0])/2):len(train[0])])\n",
    "  main_label1=np.array(label)\n",
    "  count1 = np.count_nonzero(main_label1 == 1)\n",
    "  count0 = np.count_nonzero(main_label1 == 0)\n",
    "  print(count1 , count0)\n",
    "\n",
    "  P = siamese_net.fit([main_trainL1, main_trainR1],main_label1, epochs= Epoch,batch_size=128, callbacks=history, verbose=0)\n",
    "  for j in range(1000):\n",
    "    C=1\n",
    "    Before=int(P.history['loss'][-1]*100)\n",
    "    for i in range(2,Epoch+1):\n",
    "      if  int(P.history['loss'][-i]*100)==Before:\n",
    "        C=C+1\n",
    "      else:\n",
    "        C=1\n",
    "      Before=int(P.history['loss'][-i]*100)\n",
    "    if C==Epoch:\n",
    "      break\n",
    "\n",
    "    train,label=MakeDatasetforTrain(main_train1p,main_train0p,main_trainn,main_label1p,main_label0p,main_labeln)\n",
    "    main_trainL1=np.array(train[:,0:int(len(train[0])/2)])\n",
    "    main_trainR1=np.array(train[:,int(len(train[0])/2):len(train[0])])\n",
    "    main_label1=np.array(label)\n",
    "    P = siamese_net.fit([main_trainL1, main_trainR1], main_label1,batch_size=128, epochs=Epoch, callbacks=history, verbose=0)\n",
    "    print(j+1)\n",
    "  return(siamese_net)\n",
    "\n",
    "def InternalTest(x1_test,x0_test):\n",
    "  dataset_test2=[]    #concat test\n",
    "  label_test2=[]\n",
    "  for i in range(len(x0_test)):\n",
    "    for j in range(i+1,len(x0_test)):\n",
    "      dataset_test2.append(np.concatenate((x0_test[i],x0_test[j]), axis=None))\n",
    "      label_test2.append(1)\n",
    "\n",
    "  for i in range(len(x1_test)):\n",
    "    for j in range(i+1,len(x1_test)):\n",
    "      dataset_test2.append(np.concatenate((x1_test[i],x1_test[j]), axis=None))\n",
    "      label_test2.append(1)\n",
    "\n",
    "  for i in range(len(x0_test)):\n",
    "    for j in range(len(x1_test)):\n",
    "      dataset_test2.append(np.concatenate((x0_test[i],x1_test[j]), axis=None))\n",
    "      label_test2.append(0)\n",
    "  return(np.asarray(dataset_test2),np.asarray(label_test2))\n",
    "\n",
    "def Evaluatesimilartiy(x1_test,x0_test,siamese_net):\n",
    "  main_test2,main_label_test2=InternalTest(x1_test,x0_test)\n",
    "  y_pred=siamese_net.predict([np.array(main_test2[:,0:int(len(main_test2[0])/2)]),np.array(main_test2[:,int(len(main_test2[0])/2):len(main_test2[0])])])\n",
    "\n",
    "  #Accuracy(main_label_test2, 1-y_pred,0.5)\n",
    "  Accuracy(main_label_test2, y_pred,0.5)\n",
    "\n",
    "\n",
    "def ExternalTest(x0_test,x1_test,x0_train,x1_train):\n",
    "  dataset_test1=[] # concat train\n",
    "  label_test1=[]\n",
    "  Indexsample=[]\n",
    "  for i in range(len(x0_test)):\n",
    "    for j in range(len(x0_train)):\n",
    "      dataset_test1.append(np.concatenate((x0_test[i],x0_train[j]), axis=None))\n",
    "      label_test1.append(1)\n",
    "      Indexsample.append(str(i)+\",0,0\")   #index in test,x0_test,x0_train\n",
    "  for i in range(len(x0_test)):\n",
    "    for j in range(len(x1_train)):\n",
    "      dataset_test1.append(np.concatenate((x0_test[i],x1_train[j]), axis=None))\n",
    "      label_test1.append(0)\n",
    "      Indexsample.append(str(i)+\",0,1\")   #index in test,x0_test,x1_train\n",
    "  for i in range(len(x1_test)):\n",
    "    for j in range(len(x0_train)):\n",
    "      dataset_test1.append(np.concatenate((x1_test[i],x0_train[j]), axis=None))\n",
    "      label_test1.append(0)\n",
    "      Indexsample.append(str(i)+\",1,0\")  #index in test,x1_test,x0_train\n",
    "  for i in range(len(x1_test)):\n",
    "    for j in range(len(x1_train)):\n",
    "      dataset_test1.append(np.concatenate((x1_test[i],x1_train[j]), axis=None))\n",
    "      label_test1.append(1)\n",
    "      Indexsample.append(str(i)+\",1,1\")  #index in test,x1_test,x1_train\n",
    "  return(np.asarray(dataset_test1),np.asarray(label_test1),Indexsample)\n",
    "\n",
    "def EvaluateMP(x0_test,x1_test,x0_train,x1_train,siamese_net,cutoff):\n",
    "  main_test1,main_label_test1,Indexsample=ExternalTest(x0_test,x1_test,x0_train,x1_train)\n",
    "  y_pred=siamese_net.predict([np.array(main_test1[:,0:int(len(main_test1[0])/2)]),np.array(main_test1[:,int(len(main_test1[0])/2):len(main_test1[0])])])\n",
    "  Pred,Label=DefineTest1(x0_test,x1_test,Indexsample,y_pred)\n",
    "  Accuracy(Label, Pred,cutoff)\n",
    "\n",
    "  y_pred=siamese_net.predict([np.array(main_test1[:,0:int(len(main_test1[0])/2)]),np.array(main_test1[:,int(len(main_test1[0])/2):len(main_test1[0])])])\n",
    "  Pred1,Pred,Label=DefineTest2(x0_test,x1_test,Indexsample,y_pred)\n",
    "  Accuracy1(Label, np.array(Pred),np.array(Pred1))\n",
    "\n",
    "  y_pred=siamese_net.predict([np.array(main_test1[:,0:int(len(main_test1[0])/2)]),np.array(main_test1[:,int(len(main_test1[0])/2):len(main_test1[0])])])\n",
    "  Pred1,Pred,Label=DefineTest3(x0_test,x1_test,Indexsample,y_pred)\n",
    "  Accuracy1(Label, np.array(Pred),np.array(Pred1))\n",
    "\n",
    "  y_pred=siamese_net.predict([np.array(main_test1[:,0:int(len(main_test1[0])/2)]),np.array(main_test1[:,int(len(main_test1[0])/2):len(main_test1[0])])])\n",
    "  Pred1,Pred,Label=DefineTest4(x0_test,x1_test,Indexsample,y_pred, len(x0_train)//10, len(x1_train)//10)\n",
    "  Accuracy1(Label, np.array(Pred),np.array(Pred1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d90082f-8f34-4b43-ac3a-d5ce545167e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CommonSeqbwteentwokeys(dict1,dict2):\n",
    "  common_keys = set(dict1.keys()) & set(dict2.keys())\n",
    "\n",
    "# Result\n",
    "  for key in common_keys:\n",
    "    del dict1[key]\n",
    "    del dict2[key]\n",
    "  return(dict1,dict2)\n",
    "\n",
    "def CommonSeqbwteentwokeys1(dict1,dict2):\n",
    "  common_keys = set(dict1.keys()) & set(dict2.keys())\n",
    "\n",
    "# Result\n",
    "  for key in common_keys:\n",
    "    del dict2[key]\n",
    "  return(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124bdec-6b9c-49f2-ae05-26223ce34730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(main_train1p,main_train0p,main_trainn,main_labeln, main_label1p, main_label0p,Epoch,siamese_net):\n",
    "\n",
    "  history=History()\n",
    "  train,label=MakeDatasetforTrain(main_train1p,main_train0p,main_trainn,main_label1p,main_label0p,main_labeln)\n",
    "  main_trainL1=np.array(train[:,0:int(len(train[0])/2)])\n",
    "  main_trainR1=np.array(train[:,int(len(train[0])/2):len(train[0])])\n",
    "  main_label1=np.array(label)\n",
    "  count1 = np.count_nonzero(main_label1 == 1)\n",
    "  count0 = np.count_nonzero(main_label1 == 0)\n",
    "  print(count1 , count0)\n",
    "\n",
    "  P = siamese_net.fit([main_trainL1, main_trainR1],main_label1, epochs= Epoch,batch_size=128, callbacks=history, verbose=0)\n",
    "  for j in range(100):\n",
    "    C=1\n",
    "    Before=int(P.history['loss'][-1]*10)\n",
    "    for i in range(2,Epoch+1):\n",
    "      if  int(P.history['loss'][-i]*10)==Before:\n",
    "        C=C+1\n",
    "      else:\n",
    "        C=1\n",
    "      Before=int(P.history['loss'][-i]*10)\n",
    "    if C==Epoch:\n",
    "      break\n",
    "\n",
    "    train,label=MakeDatasetforTrain(main_train1p,main_train0p,main_trainn,main_label1p,main_label0p,main_labeln)\n",
    "    main_trainL1=np.array(train[:,0:int(len(train[0])/2)])\n",
    "    main_trainR1=np.array(train[:,int(len(train[0])/2):len(train[0])])\n",
    "    main_label1=np.array(label)\n",
    "    P = siamese_net.fit([main_trainL1, main_trainR1], main_label1,batch_size=128, epochs=Epoch, callbacks=history, verbose=0)\n",
    "    print(j+1)\n",
    "  return(siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1389e597-f04a-42ba-867a-73f0b4de1b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "def contrastive_loss(y_true, y_pred, margin=1.0):\n",
    "    squared_pred = K.square(y_pred)\n",
    "    margin_squared = K.square(K.maximum(margin - y_pred, 0))\n",
    "    return K.mean(y_true * squared_pred + (1 - y_true) * margin_squared)\n",
    "\n",
    "\n",
    "def cosine_contrastive_loss(y_true, embeddings1, embeddings2, margin=0.5):\n",
    "    cos_sim = tf.keras.losses.cosine_similarity(embeddings1, embeddings2)\n",
    "    loss_pos = y_true * tf.square(1 - cos_sim)\n",
    "    loss_neg = (1 - y_true) * tf.square(tf.maximum(cos_sim - margin, 0))\n",
    "    return tf.reduce_mean(loss_pos + loss_neg)\n",
    "def cosine_similarity(vectors):\n",
    "    x, y = vectors\n",
    "    x = tf.nn.l2_normalize(x, axis=1)\n",
    "    y = tf.nn.l2_normalize(y, axis=1)\n",
    "    return tf.reduce_sum(x * y, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def euclidean_distance(vectors):\n",
    "    x, y = vectors\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "def Model1(l):\n",
    "  input_shape1=(l,)\n",
    "  left_input = Input(shape=(l,))\n",
    "  right_input = Input(shape=(l,))\n",
    "\n",
    "  hidden_size11=128\n",
    "  hidden_size12=64\n",
    "  hidden_size13=32\n",
    "  hidden_size14=16\n",
    "\n",
    "\n",
    "  model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_size11,input_shape=input_shape1,  activation=swish,kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(hidden_size12, activation=swish,kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(hidden_size13, activation=swish,kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(hidden_size14, activation=swish,kernel_regularizer=regularizers.l2(0.001)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "\n",
    "  ])\n",
    "\n",
    "\n",
    "  encoded_l = model(left_input)\n",
    "  encoded_r = model(right_input)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  L1 = tf.keras.layers.Lambda(lambda x: K.abs(x[0] - x[1]))([encoded_l, encoded_r])\n",
    "  L2 = tf.keras.layers.Multiply()([encoded_l, encoded_r])\n",
    "  merged = tf.keras.layers.Concatenate()([L1, L2])\n",
    "  L3 = tf.keras.layers.Dense(16, activation=swish,kernel_regularizer=regularizers.l2(0.001))(merged)\n",
    "  tf.keras.layers.BatchNormalization()(L3)\n",
    "  L3_D=tf.keras.layers.Dropout(0.2)(L3)\n",
    "  out = tf.keras.layers.Dense(1, activation='sigmoid')(L3_D)\n",
    "  siamese_net = tf.keras.Model([left_input, right_input], out)\n",
    "  optimizer= Adam(learning_rate=0.0001)\n",
    "  siamese_net.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\", \"mae\", \"mse\"])\n",
    "  siamese_net.summary()\n",
    "  return(siamese_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85726eed-4607-4d1d-8a82-24402a51b480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/MPFit/ESM/ESM_MP.csv')\n",
    "df2 = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/MPFit/ESM/ESM_Non_MP.csv')\n",
    "\n",
    "df1 = df1.drop(columns=['sequence_id','sequence'])\n",
    "df2 = df2.drop(columns=['sequence_id','sequence'])\n",
    "df1['label'] = 1\n",
    "df2['label'] = 0\n",
    "\n",
    "df_combined = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# X_ext = df_combined.iloc[:,2:-1].values  # All columns except the last (label)\n",
    "# y_ext = df_combined['label'].values\n",
    "\n",
    "X = df_combined.iloc[:,:-1].values\n",
    "y = df_combined['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df3f12c-cbbb-444e-88fd-a9d012877d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_combined[feature_columns].values\n",
    "y = df_combined['label'].map({'MP': 1, 'Non-MP': 0}).values\n",
    "\n",
    "X1 = X[y == 1]  # Features where label is MP (1)\n",
    "X0 = X[y == 0]  # Features where label is Non-MP (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d5fabd-3837-49f6-8c9f-5678c543d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/MPFit/ESM/ESM_MP.csv')\n",
    "df2 = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/MPFit/ESM/ESM_Non_MP.csv')\n",
    "\n",
    "df1['label'] = 1\n",
    "df2['label'] = 0\n",
    "\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "feature_columns = df.columns[2:]\n",
    "y = df['label'].values\n",
    "df = df.drop(columns=['sequence_id','label','sequence'])\n",
    "X = df.values\n",
    "\n",
    "X1 = X[y == 1]  # Features where label is MP (1)\n",
    "X0 = X[y == 0]  # Features where label is Non-MP (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73652a-b97d-441e-b3ea-6efb8e05c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "all_fold_metrics = []\n",
    "for fold, ((train1_index, test1_index), (train0_index, test0_index)) in enumerate(zip(kf.split(X1), kf.split(X0)), 1):\n",
    "    print(f\"\\n{'='*15} Fold {fold} {'='*15}\")\n",
    "\n",
    "    x1_train = X1[train1_index]\n",
    "    x1_test = X1[test1_index]\n",
    "    x0_train = X0[train0_index]\n",
    "    x0_test = X0[test0_index]\n",
    "\n",
    "    main_train1p, main_train0p, main_trainn, main_labeln, main_label1p, main_label0p = Makedatasetfor5fold(\n",
    "        x1_train, x0_train\n",
    "    )\n",
    "\n",
    "    sim_net = Model1(len(X0[0]))\n",
    "\n",
    "    sim_net = training(main_train1p, main_train0p, main_trainn, main_labeln, main_label1p, main_label0p, 10, sim_net)\n",
    "\n",
    "    similarity_metrics, preds_scores, true_labels = Evaluatesimilartiy(x1_test, x0_test, sim_net)\n",
    "    mp_metrics = EvaluateMP(x0_test, x1_test, x0_train, x1_train, sim_net, 0.5)\n",
    "\n",
    "    threshold = 0.5 # Convert continuous similarity scores to binary labels\n",
    "    preds_binary = (preds_scores >= threshold).astype(int)\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(true_labels, preds_binary)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate fold metrics\n",
    "    fold_metrics = {\n",
    "        'Fold': fold,\n",
    "        'TP': tp,\n",
    "        'TN': tn,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'Accuracy': accuracy_score(true_labels, preds_binary),\n",
    "        'Precision': precision_score(true_labels, preds_binary),\n",
    "        'Recall': recall_score(true_labels, preds_binary),\n",
    "        'F1_Score': f1_score(true_labels, preds_binary),\n",
    "        **similarity_metrics, # Include metrics from Evaluatesimilartiy\n",
    "        **mp_metrics          # Include metrics from EvaluateMP\n",
    "    }\n",
    "\n",
    "    all_fold_metrics.append(fold_metrics)\n",
    "    print(f\"  Fold {fold} CM:\\n{cm}\")\n",
    "    print(f\"  Fold {fold} F1: {fold_metrics['F1_Score']:.4f}, Acc: {fold_metrics['Accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa40a7d-9992-4763-8d48-9ac81f2ea6e3",
   "metadata": {},
   "source": [
    "# Entire Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293f519-9749-4455-9bf2-9ce1cef494e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Lambda, Multiply, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92e284a-1582-44ef-a6f0-a342f3a7a2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/MPFit/ESM/ESM_MP.csv')\n",
    "df2 = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/MPFit/ESM/ESM_Non_MP.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b379f688-612b-415e-9de2-6db0eec82962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    return x * K.sigmoid(x)\n",
    "\n",
    "def Add_max(max, value, L):\n",
    "    if len(max) < L:\n",
    "        max.append(value)\n",
    "    else:\n",
    "        min_index = max.index(min(max))\n",
    "        if max[min_index] < value:\n",
    "            max[min_index] = value\n",
    "    return max\n",
    "\n",
    "def DefineTest4(x0_test, x1_test, Indexsample, y_pred, L0, L1):\n",
    "    Pred = []\n",
    "    Pred1 = []\n",
    "    Label = []\n",
    "    for i in range(len(x0_test)):\n",
    "        max1 = []\n",
    "        max0 = []\n",
    "        for j in range(len(Indexsample)):\n",
    "            triplet = Indexsample[j].split(\",\")\n",
    "            if int(triplet[0]) == i:\n",
    "                if int(triplet[1]) == 0:\n",
    "                    if int(triplet[2]) == 1:\n",
    "                        max1 = Add_max(max1, y_pred[j], L1)\n",
    "                    if int(triplet[2]) == 0:\n",
    "                        max0 = Add_max(max0, y_pred[j], L0)\n",
    "        avg1 = np.mean(max1)\n",
    "        avg0 = np.mean(max0)\n",
    "        if avg0 > avg1:\n",
    "            Pred.append(0)\n",
    "        else:\n",
    "            Pred.append(1)\n",
    "        Pred1.append((avg1 + 1 - avg0) / 2)\n",
    "        Label.append(0)\n",
    "\n",
    "    for i in range(len(x1_test)):\n",
    "        max0 = []\n",
    "        max1 = []\n",
    "        for j in range(len(Indexsample)):\n",
    "            triplet = Indexsample[j].split(\",\")\n",
    "            if int(triplet[0]) == i:\n",
    "                if int(triplet[1]) == 1:\n",
    "                    if int(triplet[2]) == 1:\n",
    "                        max1 = Add_max(max1, y_pred[j], L1)\n",
    "                    if int(triplet[2]) == 0:\n",
    "                        max0 = Add_max(max0, y_pred[j], L0)\n",
    "        avg1 = np.mean(max1)\n",
    "        avg0 = np.mean(max0)\n",
    "        if avg0 > avg1:\n",
    "            Pred.append(0)\n",
    "        else:\n",
    "            Pred.append(1)\n",
    "        Pred1.append((avg1 + 1 - avg0) / 2)\n",
    "        Label.append(1)\n",
    "    return Pred1, Pred, Label\n",
    "\n",
    "def ExternalTest(x0_test, x1_test, x0_train, x1_train):\n",
    "    dataset_test1 = []\n",
    "    label_test1 = []\n",
    "    Indexsample = []\n",
    "    for i in range(len(x0_test)):\n",
    "        for j in range(len(x0_train)):\n",
    "            dataset_test1.append(np.concatenate((x0_test[i], x0_train[j]), axis=None))\n",
    "            label_test1.append(1)\n",
    "            Indexsample.append(str(i) + \",0,0\")\n",
    "    for i in range(len(x0_test)):\n",
    "        for j in range(len(x1_train)):\n",
    "            dataset_test1.append(np.concatenate((x0_test[i], x1_train[j]), axis=None))\n",
    "            label_test1.append(0)\n",
    "            Indexsample.append(str(i) + \",0,1\")\n",
    "    for i in range(len(x1_test)):\n",
    "        for j in range(len(x0_train)):\n",
    "            dataset_test1.append(np.concatenate((x1_test[i], x0_train[j]), axis=None))\n",
    "            label_test1.append(0)\n",
    "            Indexsample.append(str(i) + \",1,0\")\n",
    "    for i in range(len(x1_test)):\n",
    "        for j in range(len(x1_train)):\n",
    "            dataset_test1.append(np.concatenate((x1_test[i], x1_train[j]), axis=None))\n",
    "            label_test1.append(1)\n",
    "            Indexsample.append(str(i) + \",1,1\")\n",
    "    return np.asarray(dataset_test1), np.asarray(label_test1), Indexsample\n",
    "\n",
    "def makedatasetPos(data):\n",
    "    dataset = []\n",
    "    label = []\n",
    "    for i in range(len(data)):\n",
    "        for j in range(i + 1, len(data)):\n",
    "            dataset.append(np.concatenate((data[i], data[j]), axis=None))\n",
    "            label.append(1)\n",
    "            dataset.append(np.concatenate((data[j], data[i]), axis=None))\n",
    "            label.append(1)\n",
    "    return dataset, label\n",
    "\n",
    "def makedatasetNeg(data1, data2):\n",
    "    dataset = []\n",
    "    label = []\n",
    "    for i in range(len(data1)):\n",
    "        for j in range(len(data2)):\n",
    "            dataset.append(np.concatenate((data1[i], data2[j]), axis=None))\n",
    "            label.append(0)\n",
    "            dataset.append(np.concatenate((data2[j], data1[i]), axis=None))\n",
    "            label.append(0)\n",
    "    return dataset, label\n",
    "\n",
    "def MakedatasetTotal(x1_train, x0_train):\n",
    "    dataset1p, label1p = makedatasetPos(x1_train)\n",
    "    dataset0p, label0p = makedatasetPos(x0_train)\n",
    "    datasetn, labeln = makedatasetNeg(x0_train, x1_train)\n",
    "    main_train1p = np.asarray(dataset1p)\n",
    "    main_train0p = np.asarray(dataset0p)\n",
    "    main_trainn = np.asarray(datasetn)\n",
    "    main_labeln = np.asarray(labeln)\n",
    "    main_label1p = np.asarray(label1p)\n",
    "    main_label0p = np.asarray(label0p)\n",
    "    return main_train1p, main_train0p, main_trainn, main_labeln, main_label1p, main_label0p\n",
    "\n",
    "def MakeDatasetforTrain(P1, P0, N, LP1, LP0, LN):\n",
    "    TempP0 = []\n",
    "    TempL0 = []\n",
    "    i = 0\n",
    "    while i < len(P1):\n",
    "        l = random.randint(0, len(P0) - 1)\n",
    "        TempP0.append(P0[l])\n",
    "        TempL0.append(LP0[l])\n",
    "        i += 1\n",
    "    print('P1', len(P1), 'P0', len(TempP0))\n",
    "    train = np.copy(np.append(P1, TempP0, axis=0))\n",
    "    label = np.copy(np.append(LP1, TempL0))\n",
    "\n",
    "    TempN = []\n",
    "    TempLN = []\n",
    "    while len(TempN) < len(train):\n",
    "        l = random.randint(0, len(N) - 1)\n",
    "        TempN.append(N[l])\n",
    "        TempLN.append(LN[l])\n",
    "\n",
    "    train = np.copy(np.append(train, TempN, axis=0))\n",
    "    label = np.copy(np.append(label, TempLN))\n",
    "\n",
    "    return train, label\n",
    "\n",
    "def training(main_train1p, main_train0p, main_trainn, main_labeln, main_label1p, main_label0p, Epoch, siamese_net):\n",
    "    history = tf.keras.callbacks.History()\n",
    "    train, label = MakeDatasetforTrain(main_train1p, main_train0p, main_trainn, main_label1p, main_label0p, main_labeln)\n",
    "    main_trainL1 = np.array(train[:, 0:int(len(train[0]) / 2)])\n",
    "    main_trainR1 = np.array(train[:, int(len(train[0]) / 2):len(train[0])])\n",
    "    main_label1 = np.array(label)\n",
    "    count1 = np.count_nonzero(main_label1 == 1)\n",
    "    count0 = np.count_nonzero(main_label1 == 0)\n",
    "    print(f\"Training data - Positives: {count1}, Negatives: {count0}\")\n",
    "\n",
    "    P = siamese_net.fit([main_trainL1, main_trainR1], main_label1, epochs=Epoch, batch_size=128, callbacks=[history], verbose=1)\n",
    "\n",
    "    for j in range(100):\n",
    "        C = 1\n",
    "        Before = int(P.history['loss'][-1] * 10)\n",
    "        for i in range(2, Epoch + 1):\n",
    "            if int(P.history['loss'][-i] * 10) == Before:\n",
    "                C += 1\n",
    "            else:\n",
    "                C = 1\n",
    "            Before = int(P.history['loss'][-i] * 100)\n",
    "        if C == Epoch:\n",
    "            break\n",
    "\n",
    "        train, label = MakeDatasetforTrain(main_train1p, main_train0p, main_trainn, main_label1p, main_label0p, main_labeln)\n",
    "        main_trainL1 = np.array(train[:, 0:int(len(train[0]) / 2)])\n",
    "        main_trainR1 = np.array(train[:, int(len(train[0]) / 2):len(train[0])])\n",
    "        main_label1 = np.array(label)\n",
    "        P = siamese_net.fit([main_trainL1, main_trainR1], main_label1, batch_size=128, epochs=Epoch, callbacks=[history], verbose=1)\n",
    "        print(f\"Resampling iteration: {j + 1}\")\n",
    "    return siamese_net\n",
    "\n",
    "def Accuracy1(label_test, pred, pred1):\n",
    "    acc = accuracy_score(label_test, pred)\n",
    "    precision = precision_score(label_test, pred)\n",
    "    sensitivity = recall_score(label_test, pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(label_test, pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1 = f1_score(label_test, pred)\n",
    "    auc = roc_auc_score(label_test, pred1)\n",
    "    aupr = average_precision_score(label_test, pred1)\n",
    "\n",
    "    print(\"ACC  pre  recall spe F1  Auc  Aupr TN  FP  FN  TP\")\n",
    "    print(f\"{acc:.4f}  {precision:.4f}  {sensitivity:.4f}  {specificity:.4f}  \"\n",
    "          f\"{f1:.4f}  {auc:.4f}  {aupr:.4f}  {tn}  {fp}  {fn}  {tp}\")\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc, 'precision': precision, 'recall': sensitivity,\n",
    "        'specificity': specificity, 'f1': f1, 'auc': auc, 'aupr': aupr\n",
    "    }\n",
    "\n",
    "def Model1(l):\n",
    "    input_shape1 = (l,)\n",
    "    left_input = Input(shape=(l,))\n",
    "    right_input = Input(shape=(l,))\n",
    "\n",
    "    hidden_size11 = 128\n",
    "    hidden_size12 = 64\n",
    "    hidden_size13 = 32\n",
    "    hidden_size14 = 16\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Dense(hidden_size11, input_shape=input_shape1, activation=swish, kernel_regularizer=regularizers.l2(0.001)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(hidden_size12, activation=swish, kernel_regularizer=regularizers.l2(0.001)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(hidden_size13, activation=swish, kernel_regularizer=regularizers.l2(0.001)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(hidden_size14, activation=swish, kernel_regularizer=regularizers.l2(0.001)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "    ])\n",
    "\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "\n",
    "    L1 = tf.keras.layers.Lambda(lambda x: K.abs(x[0] - x[1]))([encoded_l, encoded_r])\n",
    "    L2 = tf.keras.layers.Multiply()([encoded_l, encoded_r])\n",
    "    merged = tf.keras.layers.Concatenate()([L1, L2])\n",
    "    L3 = tf.keras.layers.Dense(16, activation=swish, kernel_regularizer=regularizers.l2(0.001))(merged)\n",
    "    L3 = tf.keras.layers.BatchNormalization()(L3)\n",
    "    L3_D = tf.keras.layers.Dropout(0.2)(L3)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(L3_D)\n",
    "    siamese_net = tf.keras.Model([left_input, right_input], out)\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    siamese_net.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\", \"mae\", \"mse\"])\n",
    "    siamese_net.summary()\n",
    "    return siamese_net\n",
    "\n",
    "def EvaluateExternal(x0_test, x1_test, x0_train, x1_train, siamese_net):\n",
    "    \"\"\"Evaluate on external test set\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXTERNAL TEST SET EVALUATION\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    main_test1, main_label_test1, Indexsample = ExternalTest(x0_test, x1_test, x0_train, x1_train)\n",
    "\n",
    "    # Predict in batches to avoid memory issues\n",
    "    batch_size = 64\n",
    "    y_pred_prob = []\n",
    "\n",
    "    for i in range(0, len(main_test1), batch_size):\n",
    "        batch = main_test1[i:i + batch_size]\n",
    "        feature_dim = batch.shape[1] // 2\n",
    "        batch_pred = siamese_net.predict(\n",
    "            [batch[:, :feature_dim], batch[:, feature_dim:]],\n",
    "            verbose=0\n",
    "        )\n",
    "        y_pred_prob.extend(batch_pred.flatten())\n",
    "\n",
    "    y_pred_prob = np.array(y_pred_prob)\n",
    "\n",
    "    # Use DefineTest4 for evaluation (you can modify this to use other test functions)\n",
    "    Pred1, Pred, Label = DefineTest4(\n",
    "        x0_test, x1_test, Indexsample, y_pred_prob,\n",
    "        len(x0_train)//10, len(x1_train)//10\n",
    "    )\n",
    "\n",
    "    metrics = Accuracy1(Label, np.array(Pred), np.array(Pred1))\n",
    "    return metrics\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(\"Loading TRAINING data...\")\n",
    "    df1_train = pd.read_csv('/content/drive/MyDrive/MP_Prediction_MB/MPFit/ESM/ESM_MP.csv')\n",
    "    df2_train = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/MPFit/ESM/ESM_Non_MP.csv')\n",
    "\n",
    "    df1_train['label'] = 'MP'\n",
    "    df2_train['label'] = 'Non-MP'\n",
    "    df_train = pd.concat([df1_train, df2_train], ignore_index=True)\n",
    "\n",
    "    # df_train = df1_train\n",
    "    feature_columns = [col for col in df_train.columns if col not in ['label', 'protein_name', 'sequence', 'sequence_id']]\n",
    "\n",
    "    print(f\"Feature columns: {feature_columns}\")\n",
    "    print(f\"Number of features: {len(feature_columns)}\")\n",
    "\n",
    "    X_train = df_train[feature_columns].values\n",
    "    y_train = df_train['label'].map({'MP': 1, 'Non-MP': 0}).values\n",
    "\n",
    "    # Split into MP and Non-MP for training (ALL data used for training)\n",
    "    X1_train = X_train[y_train == 1]  # MP features\n",
    "    X0_train = X_train[y_train == 0]  # Non-MP features\n",
    "\n",
    "    print(f\"\\nTRAINING data shapes:\")\n",
    "    print(f\"MP (X1_train): {X1_train.shape}\")\n",
    "    print(f\"Non-MP (X0_train): {X0_train.shape}\")\n",
    "\n",
    "    # Load EXTERNAL TEST data\n",
    "    print(\"\\nLoading EXTERNAL TEST data...\")\n",
    "    # Replace these paths with your actual external test data paths\n",
    "    df1_test = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/Shirafkan/ESM/ESM_MP_final.csv')\n",
    "    df2_test = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/Shirafkan/ESM/ESM_Non_MP_final.csv')\n",
    "\n",
    "    # df1_test2 = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/Shirafkan/ESM/ESM_MP_final.csv')\n",
    "    # df2_test2 = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/Shirafkan/ESM/ESM_Non_MP_final.csv')\n",
    "\n",
    "    df1_test['label'] = 'MP'\n",
    "    df2_test['label'] = 'Non-MP'\n",
    "    df_test = pd.concat([df1_test, df2_test], ignore_index=True)\n",
    "\n",
    "    # df_test = df1_test\n",
    "\n",
    "    X_test = df_test[feature_columns].values\n",
    "    y_test = df_test['label'].map({'MP': 1, 'Non-MP': 0}).values\n",
    "\n",
    "    X1_test = X_test[y_test == 1]  # MP test features\n",
    "    X0_test = X_test[y_test == 0]  # Non-MP test features\n",
    "\n",
    "    print(f\"\\nEXTERNAL TEST data shapes:\")\n",
    "    print(f\"MP (X1_test): {X1_test.shape}\")\n",
    "    print(f\"Non-MP (X0_test): {X0_test.shape}\")\n",
    "\n",
    "    print(\"\\nCreating training dataset...\")\n",
    "    main_train1p, main_train0p, main_trainn, main_labeln, main_label1p, main_label0p = MakedatasetTotal(\n",
    "        X1_train, X0_train\n",
    "    )\n",
    "\n",
    "    print(\"\\nCreating and training model...\")\n",
    "    feature_dim = X1_train.shape[1]\n",
    "    siamese_net = Model1(feature_dim)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    siamese_net = training(\n",
    "        main_train1p, main_train0p, main_trainn,\n",
    "        main_labeln, main_label1p, main_label0p,\n",
    "        10, siamese_net\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating on EXTERNAL test set...\")\n",
    "    metrics = EvaluateExternal(X0_test, X1_test, X0_train, X1_train, siamese_net)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS ON EXTERNAL TEST SET\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics['recall']:.4f}\")\n",
    "    print(f\"Specificity: {metrics['specificity']:.4f}\")\n",
    "    print(f\"F1-Score:  {metrics['f1']:.4f}\")\n",
    "    print(f\"AUC:       {metrics['auc']:.4f}\")\n",
    "    print(f\"AUPR:      {metrics['aupr']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f5ab91-bd36-4842-b9ca-89a7c0074d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 1. Load and Prepare TRAINING Data (Original Steps)\n",
    "    # ==============================================================================\n",
    "    print(\"Loading TRAINING data...\")\n",
    "    # NOTE: Replace with your actual paths\n",
    "    df1_train = pd.read_csv('/content/drive/MyDrive/MP_Prediction_MB/MPFit/ESM/ESM_MP.csv')\n",
    "    df2_train = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/MPFit/ESM/ESM_Non_MP.csv')\n",
    "\n",
    "    df1_train['label'] = 'MP'\n",
    "    df2_train['label'] = 'Non-MP'\n",
    "    df_train = pd.concat([df1_train, df2_train], ignore_index=True)\n",
    "\n",
    "    feature_columns = [col for col in df_train.columns if col not in ['label', 'protein_name', 'sequence', 'sequence_id']]\n",
    "\n",
    "    X_train = df_train[feature_columns].values\n",
    "    y_train = df_train['label'].map({'MP': 1, 'Non-MP': 0}).values\n",
    "\n",
    "    X1_train = X_train[y_train == 1]  # MP features\n",
    "    X0_train = X_train[y_train == 0]  # Non-MP features\n",
    "\n",
    "    print(f\"\\nTRAINING data shapes: MP: {X1_train.shape}, Non-MP: {X0_train.shape}\")\n",
    "\n",
    "    main_train1p, main_train0p, main_trainn, main_labeln, main_label1p, main_label0p = MakedatasetTotal(\n",
    "        X1_train, X0_train\n",
    "    )\n",
    "\n",
    "    print(\"\\nCreating and training model...\")\n",
    "    feature_dim = X1_train.shape[1]\n",
    "    siamese_net = Model1(feature_dim)\n",
    "\n",
    "    # print(\"Starting training...\")\n",
    "    # siamese_net = training(\n",
    "    #     main_train1p, main_train0p, main_trainn,\n",
    "    #     main_labeln, main_label1p, main_label0p,\n",
    "    #     10, siamese_net\n",
    "    # )\n",
    "\n",
    "    print(\"\\nLoading FIRST EXTERNAL TEST data...\")\n",
    "\n",
    "    df1_test = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/DnaBinding/ESM/ESM_MP_final.csv')\n",
    "    df2_test = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/DnaBinding/ESM/ESM_Non_MP_final.csv')\n",
    "\n",
    "    df1_test['label'] = 'MP'\n",
    "    df2_test['label'] = 'Non-MP'\n",
    "    df_test = pd.concat([df1_test, df2_test], ignore_index=True)\n",
    "\n",
    "    X_test = df_test[feature_columns].values\n",
    "    y_test = df_test['label'].map({'MP': 1, 'Non-MP': 0}).values\n",
    "\n",
    "    X1_test = X_test[y_test == 1]  # MP test features\n",
    "    X0_test = X_test[y_test == 0]  # Non-MP test features\n",
    "\n",
    "    print(f\"FIRST EXTERNAL TEST data shapes: MP: {X1_test.shape}, Non-MP: {X0_test.shape}\")\n",
    "\n",
    "    print(\"\\nEvaluating on FIRST EXTERNAL test set...\")\n",
    "    metrics1 = EvaluateExternal(X0_test, X1_test, X0_train, X1_train, siamese_net)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS ON FIRST EXTERNAL TEST SET\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Accuracy:  {metrics1['accuracy']:.4f}\")\n",
    "    # ... (print other metrics1) ...\n",
    "\n",
    "    # ==============================================================================\n",
    "    # 4. Evaluate on SECOND EXTERNAL TEST set (NEW STEP)\n",
    "    # ==============================================================================\n",
    "\n",
    "    # Load the NEW external test data\n",
    "    print(\"\\n\" + \"#\"*60)\n",
    "    print(\"LOADING AND EVALUATING ON SECOND EXTERNAL TEST SET\")\n",
    "    print(\"#\"*60)\n",
    "\n",
    "    #  REPLACE THESE PATHS WITH YOUR SECOND EXTERNAL TEST DATA PATHS \n",
    "    df1_test_new = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/New_External_Dataset/ESM_MP.csv')\n",
    "    df2_test_new = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/New_External_Dataset/ESM_Non_MP.csv')\n",
    "    #  REPLACE THESE PATHS WITH YOUR SECOND EXTERNAL TEST DATA PATHS \n",
    "\n",
    "    df1_test_new['label'] = 'MP'\n",
    "    df2_test_new['label'] = 'Non-MP'\n",
    "    df_test_new = pd.concat([df1_test_new, df2_test_new], ignore_index=True)\n",
    "\n",
    "    # Use the same feature columns as the training data\n",
    "    X_test_new = df_test_new[feature_columns].values\n",
    "    y_test_new = df_test_new['label'].map({'MP': 1, 'Non-MP': 0}).values\n",
    "\n",
    "    X1_test_new = X_test_new[y_test_new == 1]  # MP test features\n",
    "    X0_test_new = X_test_new[y_test_new == 0]  # Non-MP test features\n",
    "\n",
    "    print(f\"\\nSECOND EXTERNAL TEST data shapes: MP: {X1_test_new.shape}, Non-MP: {X0_test_new.shape}\")\n",
    "\n",
    "    # Evaluate using the *already trained* siamese_net\n",
    "    print(\"\\nEvaluating on SECOND EXTERNAL test set...\")\n",
    "    metrics2 = EvaluateExternal(X0_test_new, X1_test_new, X0_train, X1_train, siamese_net)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FINAL RESULTS ON SECOND EXTERNAL TEST SET\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Accuracy:  {metrics2['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics2['precision']:.4f}\")\n",
    "    print(f\"Recall:    {metrics2['recall']:.4f}\")\n",
    "    print(f\"Specificity: {metrics2['specificity']:.4f}\")\n",
    "    print(f\"F1-Score:  {metrics2['f1']:.4f}\")\n",
    "    print(f\"AUC:       {metrics2['auc']:.4f}\")\n",
    "    print(f\"AUPR:      {metrics2['aupr']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure all necessary libraries are imported before running main()\n",
    "    # If the provided code is a single file, make sure to include all imports at the top.\n",
    "    try:\n",
    "        main()\n",
    "    except NameError as e:\n",
    "        print(f\"Error: {e}. Please ensure all libraries (pd, np, tf, etc.) are imported and all functions are defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ee3262-30fc-40fe-bc48-0f668e30892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_train = pd.read_csv('/content/drive/MyDrive/MP_Prediction_MB/MPFit/ESM/ESM_MP.csv')\n",
    "df2_train = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/MPFit/ESM/ESM_Non_MP.csv')\n",
    "\n",
    "df1_train['label'] = 'MP'\n",
    "df2_train['label'] = 'Non-MP'\n",
    "df_train = pd.concat([df1_train, df2_train], ignore_index=True)\n",
    "\n",
    "feature_columns = [col for col in df_train.columns if col not in ['label', 'protein_name', 'sequence', 'sequence_id']]\n",
    "\n",
    "X_train = df_train[feature_columns].values\n",
    "y_train = df_train['label'].map({'MP': 1, 'Non-MP': 0}).values\n",
    "\n",
    "X1_train = X_train[y_train == 1]  # MP features\n",
    "X0_train = X_train[y_train == 0]  # Non-MP features\n",
    "\n",
    "df1_test = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/DnaBinding/ESM/ESM_MP_final.csv')\n",
    "df2_test = pd.read_csv(r'/content/drive/MyDrive/MP_Prediction_MB/DnaBinding/ESM/ESM_Non_MP_final.csv')\n",
    "\n",
    "feature_columns = [col for col in df_train.columns if col not in ['label', 'protein_name', 'sequence', 'sequence_id']]\n",
    "\n",
    "df1_test['label'] = 'MP'\n",
    "df2_test['label'] = 'Non-MP'\n",
    "df_test = pd.concat([df1_test, df2_test], ignore_index=True)\n",
    "\n",
    "X_test = df_test[feature_columns].values\n",
    "y_test = df_test['label'].map({'MP': 1, 'Non-MP': 0}).values\n",
    "\n",
    "X1_test = X_test[y_test == 1]  # MP test features\n",
    "X0_test = X_test[y_test == 0]  # Non-MP test features\n",
    "main_train1p, main_train0p, main_trainn, main_labeln, main_label1p, main_label0p = MakedatasetTotal(\n",
    "    X1_train, X0_train\n",
    ")\n",
    "\n",
    "print(\"\\nCreating and training model...\")\n",
    "feature_dim = X1_train.shape[1]\n",
    "siamese_net = Model1(feature_dim)\n",
    "\n",
    "metrics1 = EvaluateExternal(X0_test, X1_test, X0_train, X1_train, siamese_net)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
